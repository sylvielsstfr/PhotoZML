{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhotoZ ML with random forest trees applied to DC2\n",
    "\n",
    "- Sylvie Dagoret-Campagne\n",
    "- creation date : March 8th 2022\n",
    "- affiliation : IJCLab/IN2P3/CNRS\n",
    "- from A. Newman, DESC DE School, 2016\n",
    "- CCIN2P3 : Use python kernel **desc**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements from LSST Science book:\n",
    "\n",
    "\n",
    "Photometric redshifts for LSST will be applied and calibrated over the redshift range $0 < z < 4$\n",
    "for galaxies to $r  \\simeq 27.5$. For the majority of science cases, such as weak lensing and BAO, a subset\n",
    "of galaxies with $i < 25.3$ will be used. For this high S/N gold standard subset over the\n",
    "redshift interval, $0 < z < 3$, the photometric redshift requirements are:\n",
    "\n",
    "- The root-mean-square scatter in photometric redshifts, $ \\sigma_z/(1+z)$, must be smaller than 0.05, with a goal of 0.02.\n",
    "- The fraction of $3\\sigma $  outliers at all redshifts must be below 10%.\n",
    "- The bias in $e_z = (z_{photo}−z_{spec})/(1+z_{spec})$ must be below 0.003 (or 0.01 for combined,analyses of weak lensing and baryon acoustic oscillations); the uncertainty in  z/(1+z) must also be known to similar accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tables as tb\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import division #avoids integer division; 1/2 = 0.5, not 0 \n",
    "import numpy as np #numpy routines\n",
    "from astropy.table import Table   #astropy routine for reading tables\n",
    "import matplotlib.pyplot as plt   #plotting routines\n",
    "\n",
    "# Random forest routine from scikit-learn:\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Cross-Validation routines:\n",
    "#from sklearn.cross_validation import KFold\n",
    "from sklearn.model_selection import KFold\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in catalog of magnitudes and redshifts\n",
    "#cat = Table.read('data_trim.fits.gz')\n",
    "\n",
    "# How big is the catalog?\n",
    "#print('Full catalog: ',len(cat),' objects')\n",
    "\n",
    "# What information does it contain for each object?\n",
    "#print(cat.colnames)\n",
    "#print(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdatadir                   = \"../data\"\n",
    "#filename_train_galaxies_h5     = \"training_100gal.hdf5\"\n",
    "filename_train_galaxies_h5     = \"test_dc2_training_9816.hdf5\"\n",
    "fullfilename_train_galaxies_h5 = os.path.join(inputdatadir,filename_train_galaxies_h5)\n",
    "\n",
    "filename_test_galaxies_h5      = \"test_dc2_validation_9816.hdf5\"\n",
    "fullfilename_test_galaxies_h5  = os.path.join(inputdatadir,filename_test_galaxies_h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5file_train = tb.open_file(fullfilename_train_galaxies_h5)\n",
    "h5file_test = tb.open_file(fullfilename_test_galaxies_h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertformat(h5file):\n",
    "    \"\"\"\n",
    "    \n",
    "    input : H5 file descriptor\n",
    "    output : datasets\n",
    "    \n",
    "    \"\"\"\n",
    "    u_mag = np.array(h5file.root.photometry.mag_u_lsst)\n",
    "    g_mag = np.array(h5file.root.photometry.mag_g_lsst)\n",
    "    r_mag = np.array(h5file.root.photometry.mag_r_lsst)\n",
    "    i_mag = np.array(h5file.root.photometry.mag_i_lsst)\n",
    "    z_mag = np.array(h5file.root.photometry.mag_z_lsst)\n",
    "    y_mag = np.array(h5file.root.photometry.mag_y_lsst)\n",
    "    u_err = np.array(h5file.root.photometry.mag_err_u_lsst)\n",
    "    g_err = np.array(h5file.root.photometry.mag_err_g_lsst)\n",
    "    r_err = np.array(h5file.root.photometry.mag_err_r_lsst)\n",
    "    i_err = np.array(h5file.root.photometry.mag_err_i_lsst)\n",
    "    z_err = np.array(h5file.root.photometry.mag_err_z_lsst)\n",
    "    y_err = np.array(h5file.root.photometry.mag_err_y_lsst)\n",
    "    z = np.array(h5file.root.photometry.redshift)\n",
    "    \n",
    "    \n",
    "    #Photometry perturbed: doubling sizes of all errors\n",
    "    #--------------------------------------------------\n",
    "    u_magn = u_mag + np.sqrt(1)*u_err*np.random.randn(len(u_mag))\n",
    "    g_magn = g_mag + np.sqrt(1)*g_err*np.random.randn(len(g_mag))\n",
    "    r_magn = r_mag + np.sqrt(1)*r_err*np.random.randn(len(r_mag))\n",
    "    i_magn = i_mag + np.sqrt(1)*i_err*np.random.randn(len(i_mag))\n",
    "    z_magn = z_mag + np.sqrt(1)*z_err*np.random.randn(len(z_mag))\n",
    "    y_magn = y_mag + np.sqrt(1)*y_err*np.random.randn(len(y_mag))\n",
    "\n",
    "    \n",
    "  \n",
    "    # First: magnitudes only\n",
    "    data_mags = np.column_stack((u_mag,g_mag,r_mag,i_mag,z_mag,y_mag))\n",
    "\n",
    "    # Next: colors only\n",
    "    data_colors = np.column_stack((u_mag-g_mag, g_mag-r_mag, r_mag-i_mag, i_mag-z_mag, z_mag-y_mag))\n",
    "\n",
    "    # Next: colors and one magnitude\n",
    "    data_colmag = np.column_stack((u_mag-g_mag, g_mag-r_mag, r_mag-i_mag, i_mag-z_mag, z_mag-y_mag, i_mag))\n",
    "    perturbed_colmag=np.column_stack((u_magn-g_magn, g_magn-r_magn, r_magn-i_magn, i_magn-z_magn, z_magn-y_magn, i_magn))\n",
    "\n",
    "    # Finally: colors, magnitude, and size\n",
    "    #data_colmagsize = np.column_stack((u_mag-g_mag, g_mag-r_mag, r_mag-i_mag, i_mag-z_mag, z_mag-y_mag, i_mag, rad))\n",
    "\n",
    "    \n",
    "    data_z = z\n",
    "    \n",
    "    return data_mags, data_colors, data_colmag, perturbed_colmag, data_z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_mags, train_data_colors, train_data_colmag, train_perturbed_colmag, train_z = convertformat(h5file_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_mags, test_data_colors, test_data_colmag, test_perturbed_colmag, test_z = convertformat(h5file_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_mags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_mags.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A function that we will call a lot: makes the zphot/zspec plot and calculates key statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function that we will call a lot: makes the zphot/zspec plot and calculates key statistics\n",
    "def plot_and_stats(zspec,zphot):\n",
    "    \n",
    "    x = np.arange(0,5.4,0.05)\n",
    "    outlier_upper = x + 0.15*(1+x)\n",
    "    outlier_lower = x - 0.15*(1+x)\n",
    "\n",
    "    mask = np.abs((z_phot - z_spec)/(1 + z_spec)) > 0.15\n",
    "    notmask = ~mask \n",
    "    \n",
    "    # Standard Deviation of the predicted redshifts compared to the data:\n",
    "    #-----------------------------------------------------------------\n",
    "    std_result = np.std((z_phot - z_spec)/(1 + z_spec), ddof=1)\n",
    "    print('Standard Deviation: %6.4f' % std_result)\n",
    "\n",
    "    # Normalized MAD (Median Absolute Deviation):\n",
    "    #------------------------------------------\n",
    "    nmad = 1.48 * np.median(np.abs((z_phot - z_spec)/(1 + z_spec)))\n",
    "    print('Normalized MAD: %6.4f' % nmad)\n",
    "\n",
    "    # Percentage of delta-z > 0.15(1+z) outliers:\n",
    "    #-------------------------------------------\n",
    "    eta = np.sum(np.abs((z_phot - z_spec)/(1 + z_spec)) > 0.15)/len(z_spec)\n",
    "    print('Delta z >0.15(1+z) outliers: %6.3f percent' % (100.*eta))\n",
    "    \n",
    "    # Median offset (normalized by (1+z); i.e., bias:\n",
    "    #-----------------------------------------------\n",
    "    bias = np.median(((z_phot - z_spec)/(1 + z_spec)))\n",
    "    sigbias=std_result/np.sqrt(0.64*len(z_phot))\n",
    "    print('Median offset: %6.3f +/- %6.3f' % (bias,sigbias))\n",
    "    \n",
    "    # make photo-z/spec-z plot\n",
    "    #------------------------\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    \n",
    "    #add lines to indicate outliers\n",
    "    plt.plot(x, outlier_upper, 'k--')\n",
    "    plt.plot(x, outlier_lower, 'k--')\n",
    "    plt.plot(z_spec[mask], z_phot[mask], 'r.', markersize=6,  alpha=0.5)\n",
    "    plt.plot(z_spec[notmask], z_phot[notmask], 'b.',  markersize=6, alpha=0.5)\n",
    "    plt.plot(x, x, linewidth=1.5, color = 'red')\n",
    "    plt.title('$\\sigma_\\mathrm{NMAD} \\ = $%6.4f\\n'%nmad+'$(\\Delta z)>0.15(1+z) $ outliers = %6.3f'%(eta*100)+'%', fontsize=18)\n",
    "    plt.xlim([0.0, 3])\n",
    "    plt.ylim([0.0, 3])\n",
    "    plt.xlabel('$z_{\\mathrm{spec}}$', fontsize = 27)\n",
    "    plt.ylabel('$z_{\\mathrm{photo}}$', fontsize = 27)\n",
    "    plt.grid(alpha = 0.8)\n",
    "    plt.tick_params(labelsize=15)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to set up an implementation of the scikit-learn RandomForestRegressor in an object called 'regrn'. \n",
    "regrn = RandomForestRegressor(n_estimators = 50, max_depth = 30, max_features = 'auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationships of galaxy properties to magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots of colors vs. redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_mag,g_mag,r_mag,i_mag,z_mag,y_mag = train_data_mags[:,0] , train_data_mags[:,1] , train_data_mags[:,2] , train_data_mags[:,3] , train_data_mags[:,4], train_data_mags[:,5]\n",
    "z = train_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "# Plots of colors vs. redshift\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "    \n",
    "    ax = fig.add_subplot(2,3,1)\n",
    "    #plt.plot(z, u_mag-g_mag, '.', color = 'red', markersize=3, alpha=0.2)\n",
    "    ext_range = [ [z.min(),z.max()], [-0.5, 2.5] ]\n",
    "    ax.hist2d(z, u_mag-g_mag,bins=(50,50),range=ext_range,cmap=\"Blues\")\n",
    "    # Axis limits:\n",
    "    #plt.xlim([0., 1.5])\n",
    "    #plt.ylim([-0.5, 2.5])\n",
    "    #Axis labels\n",
    "    ax.set_ylabel('$u-g$ color', fontsize=20)\n",
    "    ax.set_xlabel('Redshift',fontsize=20)\n",
    "    #Add grid lines\n",
    "    ax.grid()\n",
    "\n",
    "\n",
    "    #plt.figure(figsize=(8, 5))\n",
    "    ax = fig.add_subplot(2,3,2)\n",
    "\n",
    "    #plt.plot(z, g_mag-r_mag, '.', color = 'red', markersize=3, alpha=0.2)\n",
    "    ext_range = [ [z.min(),z.max()], [-0.4, 2.] ]\n",
    "    ax.hist2d(z, g_mag-r_mag,bins=(50,50),range=ext_range,cmap=\"Greens\")\n",
    "    #plt.xlim([0., 1.5])\n",
    "    #plt.ylim([-0.4, 2.])\n",
    "    ax.set_ylabel('$g-r$ color', fontsize=20)\n",
    "    ax.set_xlabel('Redshift',fontsize=20)\n",
    "    ax.grid()\n",
    "\n",
    "\n",
    "    #plt.figure(figsize=(8, 5))\n",
    "    ax = fig.add_subplot(2,3,3)\n",
    "    ext_range = [ [z.min(),z.max()], [-0.3, 1.5] ]\n",
    "    #plt.plot(z, r_mag-i_mag, '.', color = 'red', markersize=3, alpha=0.2)\n",
    "    ax.hist2d(z, r_mag-i_mag,bins=(50,50),range=ext_range,cmap=\"Reds\")\n",
    "    #plt.xlim([0., 1.5])\n",
    "    #plt.ylim([-0.3, 1.5])\n",
    "    ax.set_ylabel('$r-i$ color', fontsize=20)\n",
    "    ax.set_xlabel('Redshift',fontsize=20)\n",
    "    ax.grid()\n",
    "\n",
    "\n",
    "    #plt.figure(figsize=(8, 5))\n",
    "    ax = fig.add_subplot(2,3,4)\n",
    "    ext_range = [ [z.min(),z.max()], [-0.2, 1.1] ]\n",
    "    #plt.plot(z, i_mag-z_mag, '.', color = 'red', markersize=3, alpha=0.2)\n",
    "    ax.hist2d(z, i_mag-z_mag,bins=(50,50),range=ext_range,cmap=\"Oranges\")\n",
    "    #plt.xlim([0., 1.5])\n",
    "    #plt.ylim([-0.2, 1.1])\n",
    "    ax.set_ylabel('$i-z$ color', fontsize=20)\n",
    "    ax.set_xlabel('Redshift',fontsize=20)\n",
    "    ax.grid()\n",
    "\n",
    "    #plt.figure(figsize=(8, 5))\n",
    "    ax = fig.add_subplot(2,3,5)\n",
    "    ext_range = [ [z.min(),z.max()], [-0.5, 0.8] ]\n",
    "    #plt.plot(z, z_mag-y_mag, '.', color = 'red', markersize=3, alpha=0.2)\n",
    "    ax.hist2d(z, i_mag-z_mag,bins=(50,50),range=ext_range,cmap=\"Greys\")\n",
    "    #plt.xlim([0., 1.5])\n",
    "    #plt.ylim([-0.5, 0.8])\n",
    "    ax.set_ylabel('$z-y$ color', fontsize=20)\n",
    "    ax.set_xlabel('Redshift',fontsize=20)\n",
    "    ax.grid()\n",
    "\n",
    "    #plt.figure(figsize=(8, 5))\n",
    "    ax = fig.add_subplot(2,3,6)\n",
    "    ext_range = [ [z.min(),z.max()], [17, 26] ]\n",
    "    #Plot of i magnitude vs. size\n",
    "    #plt.plot(z, i_mag, '.', color = 'red', markersize=3, alpha=0.5)\n",
    "    ax.hist2d(z, i_mag,bins=(50,50),range=ext_range,cmap=\"GnBu\")\n",
    "    #ax.set_xlim([0., 2.])\n",
    "    #plt.set_ylim([15.0, 28])\n",
    "    ax.set_ylabel('$i$ magnitude',fontsize=20)\n",
    "    ax.set_xlabel('Redshift',fontsize=20)\n",
    "    ax.grid()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running photo-z code and testing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest - 50/50 training/test split\n",
    "\n",
    "To start, we'll split our data in two randomly, train with 50%, and test with the other 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "\n",
    "    # To better assess the quality of the Random Forest fitting, \n",
    "    # we split the data into Training (50%) and Test (50%) sets. \n",
    "    # The code below performs this task on the data_mags and data_z arrays:\n",
    "    #data_train, data_test, z_train, z_test = train_test_split(data_mags, data_z, \n",
    "    #                    test_size = 0.50, train_size = 0.50, random_state=7182016)\n",
    "    \n",
    "    data_train = train_data_mags\n",
    "    z_train    = train_z\n",
    "    \n",
    "    data_test = test_data_mags\n",
    "    z_test    = test_z  \n",
    "\n",
    "    #Train the regressor using the training data\n",
    "    regrn.fit(data_train,z_train)\n",
    "\n",
    "    #Apply the regressor to predict values for the test data\n",
    "    z_phot = regrn.predict(data_test)\n",
    "    z_spec = z_test\n",
    "\n",
    "    #Make a photo-z/spec-z plot and output summary statistics.\n",
    "    plot_and_stats(z_spec,z_phot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beware: Do not test with the same objects you use to train an algorithm !!!\n",
    "\n",
    "Let's try it to see why not..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "\n",
    "    # use the regressor trained on the training set, but apply it to the training set instead of the test set\n",
    "    z_phot = regrn.predict(data_train)\n",
    "    z_spec = z_train\n",
    "\n",
    "    plot_and_stats(z_spec,z_phot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: a better, but slower, way to test: k-fold Cross-validation\n",
    "\n",
    "In k-fold cross-validation, we split the data into k subsets.  We loop over the subsets, training with all but one and testing with the other; in the end, we get the performance of training with a fraction $k-1 \\over k$ of the data, but are able to get test statistics based on the  $entire$ dataset.\n",
    "\n",
    "This is easy to do in scikit-learn, but does mean running the training k times on more data than before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    n_folds = 5\n",
    "    \n",
    "    data_mags = np.vstack([data_train,data_test])\n",
    "    data_z = np.hstack([z_train,z_test])\n",
    "\n",
    "    z_phot = cross_val_predict(regrn, data_mags,data_z, cv=n_folds)\n",
    "    z_spec = data_z\n",
    "\n",
    "    plot_and_stats(z_spec,z_phot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's try improving our photo-z's.\n",
    "\n",
    "First: use colors instead of raw magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    #set up the data splits\n",
    "    #data_train, data_test, z_train, z_test = train_test_split(data_colors, data_z, test_size = 0.50, \n",
    "    #                    train_size = 0.50, random_state=7182016)\n",
    "\n",
    "    \n",
    "    data_train = train_data_colors\n",
    "    data_test  = test_data_colors\n",
    "    \n",
    "    \n",
    "    #train the regressor\n",
    "    regrn.fit(data_train,z_train)\n",
    "\n",
    "    # run on the test set\n",
    "    z_phot = regrn.predict(data_test)\n",
    "    z_spec = z_test\n",
    "\n",
    "    plot_and_stats(z_spec,z_phot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__If you're running ahead of the class, try plotting the different magnitudes versus redshift in the code box below, and see how they relate to $z$.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's add one magnitude.\n",
    "\n",
    "This way, we will include all information we had in the original magnitudes, but in a form that takes more advantage of the regularities in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    #set up the data splits\n",
    "    #data_train, data_test, z_train, z_test = train_test_split(data_colmag, data_z, test_size = 0.50, \n",
    "    #                    train_size = 0.50, random_state=7182016)\n",
    "    \n",
    "    \n",
    "    data_train = train_data_colmag\n",
    "    data_test = test_data_colmag\n",
    "    \n",
    "\n",
    "    #train the regressor\n",
    "    regrn.fit(data_train,z_train)\n",
    "\n",
    "    # run on the test set\n",
    "    z_phot = regrn.predict(data_test)\n",
    "    z_spec = z_test\n",
    "\n",
    "    plot_and_stats(z_spec,z_phot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_mag.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What happens when training and test sets differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Different magnitude ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    \n",
    "    \n",
    "    \n",
    "    # split the sample at the median magnitude\n",
    "    is_bright = r_mag < 23.15\n",
    "\n",
    "    data_bright = train_data_colmag[is_bright]\n",
    "    z_bright = train_z[is_bright]\n",
    "    \n",
    "    r_mag_test = test_data_mags[:,2]\n",
    "    is_faint = r_mag_test > 23.15\n",
    "\n",
    "    data_faint=test_data_colmag[is_faint]\n",
    "    z_faint = test_z[is_faint]\n",
    "    \n",
    "\n",
    "    #train the regressor\n",
    "    regrn.fit(data_bright,z_bright)\n",
    "\n",
    "    # run on the test set\n",
    "    z_phot = regrn.predict(data_faint)\n",
    "    z_spec = z_faint\n",
    "\n",
    "    plot_and_stats(z_spec,z_phot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### while if we run it the opposite way..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    #train the regressor\n",
    "    regrn.fit(data_faint,z_faint)\n",
    "\n",
    "    # run on the test set\n",
    "    z_phot = regrn.predict(data_bright)\n",
    "    z_spec = z_bright\n",
    "\n",
    "    plot_and_stats(z_spec,z_phot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Different colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    # split the sample based on r-i color\n",
    "    is_red = r_mag - i_mag > 0.8\n",
    "\n",
    "    data_red = data_colors[is_red]\n",
    "    z_red = data_z[is_red]\n",
    "\n",
    "    data_blue=data_colors[~is_red]\n",
    "    z_blue = data_z[~is_red]\n",
    "\n",
    "    #train the regressor\n",
    "    regrn.fit(data_red,z_red)\n",
    "\n",
    "    # run on the test set\n",
    "    z_phot = regrn.predict(data_blue)\n",
    "    z_spec = z_blue\n",
    "\n",
    "    plot_and_stats(z_spec,z_phot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Different redshift ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    # split the sample based on redshift\n",
    "    is_hiz = data_z > 1\n",
    "\n",
    "    data_hiz = data_colors[is_hiz]\n",
    "    z_hiz = data_z[is_hiz]\n",
    "\n",
    "    data_lowz=data_colors[~is_hiz]\n",
    "    z_lowz = data_z[~is_hiz]\n",
    "\n",
    "    #train the regressor\n",
    "    regrn.fit(data_lowz,z_lowz)\n",
    "\n",
    "    # run on the test set\n",
    "    z_phot = regrn.predict(data_hiz)\n",
    "    z_spec = z_hiz\n",
    "\n",
    "    plot_and_stats(z_spec,z_phot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # What effect do magnitude errors have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:    \n",
    "    # let's go back to our original training/test split.\n",
    "    data_train, data_test, z_train, z_test = train_test_split(data_colmag, data_z, \n",
    "                        test_size = 0.50, train_size = 0.50, random_state=7182016)\n",
    "    \n",
    "    # we want to use the same splits for the perturbed dataset\n",
    "    perturbed_train, perturbed_test, pz_train, pz_test = train_test_split(perturbed_colmag, data_z, \n",
    "                        test_size = 0.50, train_size = 0.50, random_state=7182016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    #First, let's see what happens in best case: training set = test set (no errors)\n",
    "    regrn.fit(data_colmag,data_z)\n",
    "    z_phot=regrn.predict(data_colmag)\n",
    "    z_spec=data_z\n",
    "    plot_and_stats(z_spec,z_phot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    #Now, let's see when we add errors in this best case\n",
    "    z_phot=regrn.predict(perturbed_colmag)\n",
    "    z_spec=data_z\n",
    "    plot_and_stats(z_spec,z_phot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    #Now, let's see how much things degrade when we divide training & test properly\n",
    "    regrn.fit(data_train,z_train)\n",
    "    z_phot=regrn.predict(perturbed_test)\n",
    "    z_spec=pz_test\n",
    "    plot_and_stats(z_spec,z_phot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Compare to our original results:__\n",
    "\n",
    "    Standard Deviation: 0.0747\n",
    "    Normalized MAD: 0.0220\n",
    "    Delta z >0.15(1+z) outliers:  4.043 percent\n",
    "    Median offset: -0.000 +/-  0.001\n",
    "\n",
    "__Discuss: are our photo-z errors in this catalog dominated by measurement errors, or our training sets?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you have extra time: \n",
    "\n",
    "1) Modify `plot_and_stats()` to make plots of ${z_{phot}-z_{zspec}} \\over {1 + z_{spec}}$ as a function of redshift and as a function of $i$ magnitude.  Redo all plots.  What trends do you find in the residuals for each?\n",
    "\n",
    "2) Look at the documentation for scikit-learn regression routines and try one or two other methods.  Can you do better than the random forest results?  There's a code box below you can work with.\n",
    "\n",
    "3) Modify `plot_and_stats()` to provide the uncertainty in the standard deviation, not just the median shift, to help assess whether improvements are significant. $s (\\sigma) = { \\sigma \\over \\sqrt{2N}}$ where $\\sigma$ is the standard deviation of the values, $s$ is the uncertainty in that standard deviation, and $N$ is the number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "desc",
   "language": "python",
   "name": "desc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
